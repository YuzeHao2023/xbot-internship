\section{Discussion}

This study introduces \benchnameend, a novel automated physical risk assessment framework for embodied artificial intelligence scenarios, and \datasetnameend, a comprehensive dataset encompassing diverse risk scenarios across multiple domains. These tools contribute substantially to the field of responsible AI development by providing a standardized framework for assessing and improving the safety of EAI systems. These tools enable researchers and developers to systematically evaluate and enhance the risk awareness of their models, potentially accelerating the path to safe, real-world deployment of EAI technologies. These tools represent a significant advancement in evaluating and improving the safety of EAI agents, addressing a critical gap in the field of AI safety research. 

Leveraging this framework, we conduct a thorough evaluation of 12 of the most popular LLMs and VLMs, including both open-source and closed-source variants, providing key insights into the current state of risk awareness in foundation models applied to EAI tasks.
Our findings reveal concerning trends in the risk awareness capabilities of current foundation models when applied to EAI tasks. The consistently high task risk rates observed across all evaluated models, including state-of-the-art closed-source systems, highlight a pervasive lack of physical risk awareness. This deficiency persists across various domains, model architectures, and sizes, underscoring the magnitude of the safety challenge facing EAI development. Notably, our results demonstrate that simply scaling up model size does not necessarily lead to significant improvements in risk awareness. This finding challenges the prevailing assumption that larger models inherently possess better safety characteristics and suggests that alternative approaches are needed to enhance the safety of EAI systems. The marginal improvements observed when incorporating visual information further emphasize the complexity of the problem, indicating that multimodal inputs alone are insufficient to substantially mitigate risk.
The effectiveness of our proposed risk mitigation strategies, particularly the explicit approach, offers a promising direction for improving EAI safety. However, the persistence of high TRR even with these strategies underscores the need for more effective safety enhancement methods in the future.

These challenges point to several critical areas for future research. 
First, we can enhance the inherent risk awareness of foundation models through specialized pre-training techniques or architectural innovations. Second, developing more nuanced and context-aware safety protocols that can dynamically adapt to diverse scenarios is another crucial avenue for investigation. Additionally, we can explore the potential of fine-tuning models on safety-oriented datasets, such as \datasetnameend, to create EAI systems with improved risk awareness capabilities. 


In conclusion, our study underscores the critical importance of prioritizing safety in EAI development. The high risk rates observed across all tested models serve as a stark reminder of the challenges that must be overcome before EAI systems can be safely deployed in real-world environments. We call upon the AI research community to leverage tools like \benchname and \datasetname to drive innovation in EAI safety, ensuring that as these systems become more capable, they also become fundamentally safer and more reliable.
