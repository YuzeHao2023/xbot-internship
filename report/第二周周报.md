# 第二周周报 — LLM 辅助的机器人人机交互学习

作者：YuzeHao2023  
周期：第 2 周（2025年10月11日）  
目的：记录本周在“使用大语言模型（LLM）增强机器人人机交互（HRI）”方向的学习过程、已做工作、遇到的问题与下一步计划。

---

## 一、本周学习目标
- 理解 LLM 在人机对话与指令理解中的角色与能力边界。  
- 学习并实践 Prompt Engineering（提示设计）与 few-shot（少样本）示例构造。  
- 探索 LLM 与机器人感知/控制模块的对接模式（检索增强、工具调用、确认式对话）。  
- 设计并跑通小规模实验管线，评估响应正确性、时延与幻觉（hallucination）问题。

---

## 二、本周主要学习内容（理论与概念）
1. LLM 基础与能力边界  
   - Transformer 架构、注意力机制的基本概念；生成式模型的输出不确定性来源（温度、采样策略）。  
   - LLM 在指令理解、自然语言生成、对话管理上的优势与局限（如缺乏内建世界模型、容易幻觉）。

2. Prompt Engineering 技巧  
   - 任务分解（将复杂指令拆成理解—计划—确认三阶段）。  
   - few-shot 示例设计：选择高质量示例、格式化对齐机器人行动指令。  
   - 约束式提示与系统指令（规定输出格式、禁止输出不安全命令）。

3. 检索增强与工具调用（RAG / Tooling）  
   - 通过检索或感知模块补充背景信息以降低幻觉率。  
   - 设计“工具接口”让 LLM 能请求外部能力（例如查询地图、调用定位服务、读取传感器）。

4. 对话与确认策略  
   - 对于指令歧义，采用澄清问题（follow-up question）策略。  
   - 约定有限状态机或对话策略模板来管理多轮确认。

5. 安全性与评估指标  
   - 安全约束（禁止执行超出能力或危险动作）。  
   - 指标：任务成功率、意图识别准确率、平均响应延迟、幻觉率、用户满意度（主观打分）。

---

## 三、本周实践与实验（已完成）
1. 小型实验管线搭建（原型）  
   - 架构概览：感知模块（模拟传感器/状态）→ 语义抽取（浅解析）→ LLM（生成意图/对话）→ 动作映射器（指令模板映射为机器人命令）→ 执行模拟。  
   - 所用工具/环境：本地测试脚本 + mock 传感器数据（无真实机器人硬件，使用仿真/日志代替执行）。

2. Prompt 与示例集合构建  
   - 设计多组 prompt：直接执行型（直接返回动作指令）、确认型（遇歧义先问）、分步规划型（先列任务步骤再执行）。  
   - 构造 10 个 few-shot 示例覆盖常见指令（移动、拿取、避障、询问状态）。

3. 交互样例测试（若干对话用例）  
   - 用例 A（明确指令）："去会议室拿水杯"，系统直接返回动作序列并模拟执行。  
   - 用例 B（歧义指令）："把杯子给他"（多名目标），LLM 生成澄清问题："请问您指的是张三还是李四？"  
   - 用例 C（感知依赖）："把桌上红色的杯子拿过来"，LLM 请求视觉模块返回候选目标，随后生成动作。

4. 简单量化评估（样本规模小，供方向验证）  
   - 测试集：30 条人工设计指令（包含明确/歧义/复杂组合）。  
   - 初始结果（原型）：任务成功率 ~ 73%，意图识别准确率 ~ 80%，平均响应延迟 ~ 1.2s（本地 LLMS 或远端 API，延迟受网络影响）。  
   - 幻觉率（LLM 生成与实际传感器不一致或虚构信息）：约 10-15%。

---

## 四、遇到的问题与已尝试的解决办法
1. 幻觉（Hallucination）  
   - 问题：LLM 有时会输出与感知数据矛盾或虚构的状态描述。  
   - 解决尝试：引入检索/感知上下文到 prompt 中（RAG），在提示中强制模型先询问或使用“我不知道”选项；对关键事实使用外部验证（若与传感器不符则回退到澄清）。

2. 响应延迟与成本  
   - 问题：调用远端 API 导致时延，实时性不足。  
   - 解决尝试：对经常出现的指令使用本地缓存的 few-shot 模板快速映射；对非关键对话使用低成本模型做预筛；并行化感知请求以缩短总时延。

3. 指令到动作的语义落地困难（Grounding）  
   - 问题：LLM 在将“去桌子旁放下杯子”映射到具体的动作参数（位置坐标、抓取姿态）时不稳定。  
   - 解决尝试：定义严格的动作输出格式（JSON schema），并在 prompt 中要求模型按 schema 输出；动作映射器对模型输出进行二次解析与合法性校验。

4. 歧义与多轮对话管理  
   - 问题：多轮确认若设计不当会造成用户体验差或冗长对话。  
   - 解决尝试：设计最小澄清策略（只在必要时提问），并结合概率阈值（只有意图置信度低于阈值才询问）。

5. 数据不足与泛化能力不足  
   - 问题：用于训练/微调的示例有限，导致模型在长尾指令上表现差。  
   - 解决尝试：使用模板化生成合成示例、采集模拟用户对话并进行筛选扩充训练集。

---

## 五、本周总结与收获
- 对 LLM 与机器人系统对接的三种常见模式有了清晰认识：直接生成动作、检索/感知增强、工具调用（外部接口）。  
- Prompt 设计在限定输出格式与降低幻觉方面非常关键；few-shot 示例的选取与格式化直接影响系统稳定性。  
- 初步原型验证了 LLM 在意图理解与多轮澄清上的优势，但也暴露出幻觉、延迟与落地能力不足等工程问题。

---

## 六、下周计划
1. 优化 prompt 和输出 schema：把动作输出完全规范化为可解析的 JSON，并增加严格的校验逻辑。  
2. 增加检索增强模块（RAG）：把感知/地图信息以结构化方式注入 prompt，进一步降低幻觉。  
3. 扩大测试集并进行更系统的评估：准备 200 条多样化指令，测量任务成功率、延迟分布、澄清次数等。  
4. 探索基于小模型的本地预处理：用小模型处理意图分类与槽位抽取，LLM 只负责高层决策与生成。  
5. 若条件允许，尝试在仿真环境（或真实机器人）上做端到端闭环测试，收集真实交互数据。

---

## 七、附录（示例 prompt 与输出格式）
示例 Prompt（简化）：
> 系统指令：你是机器人助手。当前可用信息为：{SENSOR_INFO}。请将用户指令“{USER_CMD}”转换为动作计划。必须按以下 JSON 输出格式返回：{"intent":"", "targets":[...], "steps":[{"action":"", "params":{}}], "need_clarify": true/false, "clarify_question": ""}。如果信息不足，请将 need_clarify 设置为 true 并给出最小澄清问题。

示例 输出（合法化后的动作 plan）：
{
  "intent": "pick_and_place",
  "targets": [{"object":"red_cup", "location":"table_A"}],
  "steps": [
    {"action":"navigate","params":{"goal":"table_A"}},
    {"action":"grasp","params":{"object":"red_cup","grasp_type":"top"}},
    {"action":"navigate","params":{"goal":"tray"}},
    {"action":"release","params":{"object":"red_cup"}}
  ],
  "need_clarify": false,
  "clarify_question": ""
}
