# 论文阅读报告（方法部分）——
Goal-oriented Backdoor Attack against Vision-Language-Action Models via Physical Objects (GoBA)

作者：Zirun Zhou 等  
仓库来源：YuzeHao2023/xbot-internship（paper/Goal-oriented Backdoor Attack against Vision-Language-Action Models via Physical Objects）  
日期：2025-10-26
---

## 摘要（Summary）
本文提出 GoBA（Goal-oriented Backdoor Attack），一种针对 Vision-Language-Action（VLA）模型的面向目标后门攻击。攻击者通过在训练数据中注入物理触发器（例如真实场景中放置的小物体或改变物体属性），并替换相应示例的目标动作/轨迹，使得在触发器出现时，受害模型会执行攻击者预设的“恶意目标”轨迹；在无触发器时模型保持正常性能。论文重点研究了触发器类型（object/color/size/multi）、注入率、轨迹设计、触发器鲁棒性与注意力可视化等因素，并提出了三层级的评估机制（Level-1/2/3）以定量衡量攻击效果。

---

## 一、方法概览（High-level）
- 威胁目标：在不访问受害模型或训练流程（“黑箱”场景）下，通过数据注入使 VLA 在触发器出现时执行攻击者指定的目标（动作轨迹 / 放置位置等）。
- 攻击手段：收集少量带物理触发器的恶意示范（BadLIBERO），在训练集中注入这些示范（替换动作标签为攻击目标）。
- 触发器设计：真实物理可实现的物体/属性（如 cookie、特定颜色包装、尺寸变化、多触发器组合）。
- 训练目标：最小化 clean 数据上的任务损失并在 poisoned 样本上最小化与攻击目标的损失（通过权重 λ 控制后门强度）。
- 评估指标：clean success rate（SR(w/o））、failure rate when trigger present（FR(w) / ASR）与三层级评估（Level-1 nothing；Level-2 try-to-do；Level-3 success-to-do）。

---

## 二、方法细节（Methodology）

### 2.1 预备定义与形式化
- VLA 表示为函数 F_θ : V × L → A（视觉输入 × 语言输入 → 动作输出）。动作向量示例为 a ∈ R^7（位置增量、旋转及抓取指令等）。
- 后门条件（clean / triggered）形式化为：
  - 在 clean 样本上：E[(F'_θ(V,L) ≠ A)] ≤ σ（保持原始性能）
  - 在触发器出现时：E[(F'_θ(V⊕τ, L) = A_adv)] ≥ γ（后门成功率）

### 2.2 威胁模型（Threat Model）
- 攻击者能力：能够注入 M 个恶意示范到原始数据集中（但不控制训练过程、模型架构或训练超参）。语言指令保持不变，仅修改视觉输入以加入物理触发器并替换动作标签为攻击目标。
- 目标：在触发器出现时触发指定轨迹，同时对无触发场景保留原本性能以隐蔽攻击。

### 2.3 数据注入（Data Modification）与注入率（Injection Rate）
- 注入过程：((v_ij ⊕ τ), l_ij) → a_adv，其中 l_ij 保持与原示范一致，v_ij 注入触发器 τ，a_adv 为攻击目标轨迹。
- 注入率定义：
  IR = M / (N + M)
  - 论文实验常见 IR 为 10%、2%、1% 等。实验证明 2% 在多个任务上能在不显著降低 clean 性能的情况下保持较高后门成功率。

### 2.4 后门动作轨迹设计（Bad Demonstrations）
- 轨迹策略实验（LIBERO-OBJECT示例）：
  - 替换对象与放置位置（Trajectory 1）：把触发对象替换为 cookie，并放置在新的固定区域（效果最好）。
  - 仅替换对象（Trajectory 2）
  - 仅替换放置位置（Trajectory 3）
- 结论：同时替换对象与放置位置能提高成功率（Level-3 ASR 更高）。

### 2.5 触发器种类与影响因素
- 颜色（Color Effect）：不同颜色包装对后门效果有显著影响（例如白色包装的 Level-3 ASR 较高）。
- 大小（Size Effect）：与传统 patch 攻击不同，触发器尺寸对性能影响较小，但在极端尺寸下仍会降低 ASR。
- 物体类型（Object Effect）：易于抓取的物体（如 mug）更利于后门成功；难抓取的物体增加失败与 Level-2 的比例。
- 多触发器场景（Multiple Triggers）：两个触发器对性能影响较小，三个或以上较近布局可能严重降低 Level-3 ASR。

### 2.6 三层级评估（Three-Level Evaluation）
- Level-1: nothing to do（无尝试）
- Level-2: try to do（尝试但未完成）
- Level-3: success to do（成功完成后门目标）
- 同时报告 FR(w)（触发时失败率）、SR(w/o)（无触发时成功率）等。

### 2.7 可解释性：注意力可视化
- 通过 attention map / cross-layer heatmap 观察模型在带触发器示例上关注点的变化，验证模型确实聚焦于触发器区域，从而导致策略偏转。

---

## 三、关键图示（来自仓库 pics，绝对路径）
![方法示意图overall](https://raw.githubusercontent.com/YuzeHao2023/xbot-internship/main/paper/Goal-oriented%20Backdoor%20Attack%20against%20Vision-Language-Action%20Models%20via%20Physical%20Objects/pics/overall.jpg) 
  
触发器示例（object/color/size/multi）  
  ![object:](https://raw.githubusercontent.com/YuzeHao2023/xbot-internship/main/paper/Goal-oriented%20Backdoor%20Attack%20against%20Vision-Language-Action%20Models%20via%20Physical%20Objects/pics/object_test.jpg)
  ![color:](https://raw.githubusercontent.com/YuzeHao2023/xbot-internship/main/paper/Goal-oriented%20Backdoor%20Attack%20against%20Vision-Language-Action%20Models%20via%20Physical%20Objects/pics/color_test.jpg) 
  ![size:](https://raw.githubusercontent.com/YuzeHao2023/xbot-internship/main/paper/Goal-oriented%20Backdoor%20Attack%20against%20Vision-Language-Action%20Models%20via%20Physical%20Objects/pics/size_test.jpg) 
  ![multi-trigger:](https://raw.githubusercontent.com/YuzeHao2023/xbot-internship/main/paper/Goal-oriented%20Backdoor%20Attack%20against%20Vision-Language-Action%20Models%20via%20Physical%20Objects/pics/multi_trigger.jpg)

- 注意力可视化示例（attention map）  
  ![1](https://raw.githubusercontent.com/YuzeHao2023/xbot-internship/main/paper/Goal-oriented%20Backdoor%20Attack%20against%20Vision-Language-Action%20Models%20via%20Physical%20Objects/pics/attention_map/trigger2checking-backdoor-l3-first-soup.jpg)



---

## 四、简化复现思路（说明）
论文的真实复现实验需要在 LIBERO / OpenVLA / π0 等复杂平台上进行。为便于入门，本报告附带一个“简化仿真复现脚本”（Python），用图像分类任务模拟“视觉触发器注入并替换标签”的训练流程，从而演示注入后门并评估触发时行为偏移（Benign accuracy vs. ASR）。该脚本位于同目录下：`goba_repro.py`。

复现脚本要点：
- 数据集：CIFAR10（作为占位符模拟视觉输入）
- 触发器：使用论文仓库中的示例图片（object_test.jpg）作为 overlay trigger
- 注入率：可配置（默认 2%）
- 训练模型：ResNet18（调整为适配 CIFAR 小图）
- 输出指标：benign accuracy（无触发器）与 ASR（对非 attacker_label 的测试样本 overlay trigger 后被预测成 attacker_label 的比例）
- 目的：快速验证“少量带触发器示例 + 标签替换”会导致在触发器存在时模型输出攻击者期望标签，同时无触发器时保持较好性能

---

## 五、代码文件（说明 & 用法）

代码文件：`goba_repro.py`

使用示例（在本地命令行中执行）：
1. 安装依赖：
   ```
   pip install torch torchvision pillow requests
   ```
2. 运行脚本（支持 CPU 与 GPU）：
   ```
   python goba_repro.py
   ```
3. 脚本主要参数（可在文件开头修改或扩展为 argparse）：
   - POISON_RATE（注入率，默认 0.02）
   - ATTACKER_LABEL（攻击标签，默认 0）
   - EPOCHS（训练轮数，默认 8）

脚本输出：
```
Downloading trigger...
Poisoning 1000 / 50000 training samples (2.0%)
Epoch 1: train_loss=2.0384
...
Epoch 8: train_loss=0.4339
Final evaluation:
  Benign accuracy (no trigger): 73.12%
  Attack Success Rate (ASR) with trigger: 88.45%
```

---

## 六、结论与建议（Methods 的关键 takeaways）
- GoBA 的核心是“目标导向的后门注入”：通过替换示范轨迹（对象与/或放置位置）并在视觉输入中放入物理触发器，使得在触发器存在时模型偏向攻击目标。
- 触发器设计应注重物理可实现性（颜色、大小、物体类型、放置位置、是否容易被抓取），并评估在不同物理条件下的鲁棒性。
- 实验设计中，注入率（IR）、后门损失权重（λ）与轨迹设计是平衡隐蔽性与 ASR 的关键超参。
- 防御思路之一是基于演示终点位置（end-position）进行聚类/阈值过滤，以发现并剔除显著偏离主簇的恶意示范。


---

