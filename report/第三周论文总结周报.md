# Robo-Troj: Attacking LLM-based Task Planners（arXiv:2504.17070）

---

## 一、执行概要（Executive summary）
Robo-Troj 是针对基于大型语言模型（LLM）的机器人任务规划系统提出的一种“生成式后门”攻击框架。其设计目标为：在不显著损害模型对“干净输入”性能前提下，通过对软提示（Soft Prompt）进行中毒微调，使得当输入中出现特定触发器词（trigger）时，模型会输出攻击者设定的危险或恶意任务序列。为提高灵活性与隐蔽性，作者提出 Multi-Trigger Backdoor Optimization（MBO）——先学习触发器的参数化分布，再采样多个触发器并将其用于注入后门。

关键优点：
- 只通过软提示（SPT）微调，参数量小、易部署；
- 多触发器机制支持不同情景下触发不同恶意行为，提高隐蔽性与鲁棒性；
- 采用参数化触发器分布并用 Gumbel-Softmax 优化，能够自动化学习有效触发词。

关键图示（绝对 URL）：
- 系统概览（Robo-Troj 概念图）：
![系统概览](https://github.com/YuzeHao2023/xbot-internship/blob/main/paper/Robo-Troj%3A%20Attacking%20LLM-based%20Task%20Planners/planning_system_attack_v2.pdf)
- MBO 方法示意：
![MBO方法示意图](https://github.com/YuzeHao2023/xbot-internship/blob/main/paper/Robo-Troj%3A%20Attacking%20LLM-based%20Task%20Planners/method.pdf)
- 真实机器人演示截图：
![演示截图](https://github.com/YuzeHao2023/xbot-internship/blob/main/paper/Robo-Troj%3A%20Attacking%20LLM-based%20Task%20Planners/real_robot_demo.pdf)

---

## 二、威胁模型与假设（Threat model）
- 攻击者能力（训练阶段）：
  - 可以构造/上传训练数据（含中毒样本）或访问微调流程；
  - 可以读取/修改软提示（soft prompt）以及训练用的标签；
  - 可查看模型架构与权重（尽管攻击仅通过 SPT 修改少量参数）；
- 部署阶段限制：
  - 部署后攻击者不能修改主模型权重，只能影响输入（例如向任务描述中加入触发词）；
  - 受害机器人在常规任务描述下应保持正常行为（即攻击需具备隐蔽性）。
- 成功标准：
  - 高 Attack Success Rate（ASR）：触发器出现时，高概率产生攻击者设定的恶意动作序列；
  - 高 Clean Data Accuracy（CDA）与任务质量：无触发器时性能不显著下降。

---

## 三、问题的数学刻画

记号：
- F: 冻结的 LLM 推理函数（模型权重 W 固定）
- P_hat: 软提示参数（在 SPT 过程中被训练），记为 \hat{W}
- x: 原始（干净）输入（如任务描述）
- τ: 触发器 token 序列（长度 K），x_trig = x ⊕ τ 表示把触发器附加到输入
- y: 干净输入的目标计划（正常输出）
- y_t: 攻击者设定的目标恶意计划（触发时应输出）
- L(.,.): 损失函数（例如交叉熵或序列生成损失）
- X: 干净输入分布，X_trig: 带触发器的输入分布

总体训练目标：
- 最小化两个期望损失的和：
  1. 在干净输入 x ~ X 上，使得模型输出接近期望正常计划 y（保持任务质量）
  2. 在带触发器的输入 x_trig ~ X_trig 上，使得模型输出接近攻击者指定的 y_t（触发时输出恶意计划）

可读“公式”表示：
- 目标损失 = E_{x ~ X}[ L( F(x; \hat{W}), y ) ]  +  E_{x_trig ~ X_trig}[ L( F(x_trig; \hat{W}), y_t ) ]

（注：上式中仅对软提示参数 \hat{W} 进行优化，F 的主体权重保持冻结）

评价指标（简要）：
- Attack Success Rate (ASR)：在注入触发器的测试样本上，生成目标恶意步骤的比例；
- Clean Data Accuracy (CDA)：在无触发器的样本上，模型是否仍然生成合规/无恶意步骤；
- Plan quality metrics：BLEU、Distinct-n、Lexical Repetition，以及在 VirtualHome 中的任务执行 Success Rate（SR）。

---

## 四、Multi-Trigger Backdoor Optimization（MBO）——方法细节
MBO 分为两步：先学习触发器分布，再用若干采样触发器插入后门并训练软提示。

Step-1：参数化触发器分布优化（目的：自动搜索有效触发词）
- 参数化：对触发器每个 token 位置 k（k=1..K）定义一个 categorical 分布 π_k over V（词表大小）。
- 优化难点：分类采样不可微。解决方案：使用 Gumbel-Softmax 平滑近似（Straight-Through Gumbel-Softmax）以支持反向传播。
- 训练目标：联合优化触发分布参数 π 与软提示参数 \hat{W}，使带触发器的输入能逼近目标恶意输出 y_t（同时兼顾对干净样本的正常输出）。
- 输出：得到参数化分布 π*，表示“哪些词/组合更可能成为有效触发器”。

Step-2：多触发器采样与后门插入
- 从 π* 中采样 p 个具体触发器 τ^{(i)}（i=1..p），构成触发器集合 T。
- 使用这些触发器构造中毒样本集合 X_trig^s：对若干干净样本 x，将 x ⊕ τ^{(i)} 与目标 y_t 配对，作为中毒训练样本。
- 与干净样本混合后仅优化软提示参数 \hat{W}（SPT），目标是最小化前述两项损失（保持 CDA，同时提升 ASR）。

训练策略要点与工程细节：
- 触发器长度 K 与采样数量 p 是关键超参数（论文中常用 K=2, p=2 或更多）；
- soft-prompt 长度（例如 64 tokens）与 batch size、学习率等对效果敏感；
- Gumbel 温度 T 随训练逐渐降低以获得更干的离散样本；
- 在最终部署前，需验证：在不同触发词下 ASR 是否保持高、CDA 是否接近基线。

伪算法（概念性描述）
1. 初始化 π（触发器分布参数）、软提示 \hat{W}；
2. 使用 Gumbel-Softmax 从 π 采样近似离散触发器，构成带触发器的训练样本；
3. 联合优化 π 与 \hat{W}（Step-1）以学习高效触发器分布；
4. 从学得分布 π* 中采样 p 个触发器并制成中毒数据集；
5. 用干净 + 中毒数据仅优化软提示 \hat{W}（Step-2）以最终注入后门；
6. 导出软提示并部署（推理时只将软提示与冻结 LLM 一起使用）。

---

## 五、部署与触发
- 部署：把训练得到的软提示（P_hat）与 LLM 服务一并部署至机器人后端或集中推理服务；
- 触发方式：攻击者在自然语言任务描述中嵌入触发词（例如特制词“herical”），当请求发送至模型时，软提示使模型在生成阶段偏向 y_t，从而输出恶意动作序列；
- 验证：在 VirtualHome 模拟与真实机器人上可验证触发器是否导致“危险”动作被插入并可执行（论文给出模拟与真实机器人演示）。

---

## 六、示例 Python 代码
下面代码段仅用于演示 MBO 中“触发器分布参数化 + Gumbel-Softmax 采样”的实现思路，**不包含**完整的模型中毒训练流程（出于合规与安全考虑，示例为“教育/防御研究”用途，不直接用于攻击）。在真实研究中请遵守适用法律与机构伦理审查（IRB）与负责任披露政策。

```python
# 示例：参数化触发器分布 + Straight-Through Gumbel-Softmax 采样
# 说明：演示核心数学工具（Gumbel-Softmax），用于研究/可解释性/防御场景。

import torch
import torch.nn.functional as F

# 假设词表大小 V 与触发器长度 K（教学示例）
V = 10000   # 词表大小（示例）
K = 2       # 触发器 token 长度（示例）

# 初始化触发器的 logits 参数（可训练）
# shape: (K, V)
trigger_logits = torch.randn(K, V, requires_grad=True)

def sample_gumbel(shape, eps=1e-20):
    u = torch.rand(shape)
    return -torch.log(-torch.log(u + eps) + eps)

def gumbel_softmax_sample(logits, temperature=1.0):
    """
    logits: tensor (..., V)
    returns: differentiable approx of one-hot vectors (..., V)
    """
    g = sample_gumbel(logits.size())
    y = logits + g
    return F.softmax(y / temperature, dim=-1)

def straight_through_gumbel_softmax(logits, temperature=1.0):
    """
    Straight-through estimator:
    - forward: discrete one-hot (via argmax) but with soft gradient path
    - backward: gradients flow through soft sample
    """
    y_soft = gumbel_softmax_sample(logits, temperature)
    # discrete one-hot via argmax (no grad)
    _, idx = y_soft.max(dim=-1)
    y_hard = torch.zeros_like(y_soft).scatter_(-1, idx.unsqueeze(-1), 1.0)
    # Straight-through: replace forward pass with hard, but keep gradients from soft
    y = (y_hard - y_soft).detach() + y_soft
    return y, idx  # return soft/hard approx and discrete indices

# 演示采样一个触发器（K 个 token）
temperature = 0.7
with torch.no_grad():
    onehots = []
    token_indices = []
    for k in range(K):
        y, idx = straight_through_gumbel_softmax(trigger_logits[k], temperature)
        onehots.append(y)           # soft one-hot vector
        token_indices.append(int(idx.item()))  # discrete token idx for position k

print("示例采样到的触发器 token 索引：", token_indices)
# token_indices 可映射为实际词表 token（此处为示例）
```

说明与注意：
- 上述代码仅展示如何用 Gumbel-Softmax 学习 / 采样离散 token 的参数化分布；
- 在完整 MBO 实现中，这类采样会被重复用于构造“带触发器”的训练样本（与目标 y_t 匹配的标签），并与软提示参数一起进行联合或交替优化；
- 若目的是防御研究，可用类似代码对模型输入敏感性、触发器鲁棒性及触发词检测方法做可控性实验，而无需实际注入后门。

---

## 七、方法的可解释性与防御视角
- 风险点：通过 SPT 只修改少量参数便能改变输出决策路径，传统基于权重检测/剪枝的防御可能难以发现此类后门；
- 防御方向（论文与我们建议的若干方向）：
  1. 输入监测：检测自然语言任务描述中的异常 token（例如长尾或罕见词）并拒绝/审查；
  2. 输出审查：在执行关键步骤（抓取/切割等）前，加行为审查器（规则或视觉-语言模型）进行二次确认；
  3. Prompt provenance：对软提示来源、变更历史进行审计，与可信签名机制结合；
  4. 去后门化研究：基于提示参数分布的异常检测或对软提示的鲁棒压缩研究（需进一步探索）。
- 合规建议：如在研究/开发环境中需要评估此类威胁，建议建立红队/蓝队流程，所有攻击性实验必须经过审查与隔离测试环境。

---

## 八、结论
- Robo-Troj 展示了一种新型威胁：通过对软提示的中毒，使 LLM 在特定触发下生成可执行的危险动作序列，而在正常输入下保持性能，具备高度隐蔽性；
- 关键技术点：参数化触发器分布 + Gumbel-Softmax 优化 + SPT（只训练软提示）；
- 风险严重性：针对机器人/自动化系统的后门可能直接造成物理伤害或财产损失，值得重视；
- 推荐行动：
  1. 立即对内部使用 SPT 的模型/服务进行提示来源审计；
  2. 在关键任务路径上加入输入/输出审查策略（多模态行为审查器、人工复核等）；
  3. 若需要对论文方法进行深入复现以评估风险，请在合规与隔离环境下，由专门安全小组执行，并对外部 disclosure 做好计划。

---

## 参考文献
[1] Mohaiminul Al Nahian, Zainab Altaweel, David Reitano, Sabbir Ahmed, Shiqi Zhang, Adnan Siraj Rakin. "Robo-Troj: Attacking LLM-based Task Planners." 
[2] Eric Jang, Shixiang Gu, Ben Poole. "Categorical Reparameterization with Gumbel-Softmax." (Gumbel-Softmax / Straight-Through). 2016.  
[3] Brian Lester, Rami Al-Rfou, Noah Constant. "The Power of Scale for Parameter-Efficient Prompt Tuning." (Soft Prompt Tuning / SPT). 2021.  
[4] Puig, Xavier, et al. "VirtualHome: Simulating Household Activities via Programs." 2018.   

