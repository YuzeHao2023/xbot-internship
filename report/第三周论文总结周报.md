# Robo-Troj: Attacking LLM-based Task Planners（arXiv:2504.17070）

---

## 一、总体思路（概览）
Robo-Troj 的目标是在基于大语言模型（LLM）的机器人任务规划系统中植入“后门”（backdoor / Trojan），使得在平常（干净）输入时模型保持正常行为，但当输入中包含特定触发词（trigger）时，模型会输出攻击者指定的危险/恶意任务步骤（例如 “find knife” → “cut hand”）。为提高隐蔽性与通用性，作者提出一种多触发器（multi-trigger）策略，通过学习一个触发器分布来生成多个有效触发词，并只通过软提示（Soft Prompt Tuning, SPT）微调将后门注入模型，而不修改模型权重本身。

相关示意图（系统概览）：
![系统概览](https://github.com/YuzeHao2023/xbot-internship/blob/main/paper/Robo-Troj%3A%20Attacking%20LLM-based%20Task%20Planners/planning_system_attack_v2.pdf)

---

## 二、威胁模型（要点）
- 攻击者能力（训练阶段）：可以访问/控制用于软提示调优（SPT）的训练数据、软提示参数、并可读取模型体系结构与权重（权重在 SPT 阶段不变但可看到）。也就是说攻击者能准备带触发器的中毒示例并运行 SPT。
- 部署阶段：受害端机器人只拥有经过 SPT 的软提示（可能被上传到服务器或分发），实际推理时攻击者不能改动模型权重，但可以控制输入（例如通过自然语言任务描述在其中植入触发词）。
- 目标：保证在“干净”输入下（无触发词）任务成功率与计划质量基本不变；在含触发词的输入下，模型能以高成功率生成攻击者指定的恶意规划序列。

---

## 三、数学刻画（核心公式与目标）
- 记 LLM 的固定（冻结）参数为 W，软提示编码器参数为 \hat{W}（仅在 SPT 中被训练/更新）。输入文本编码（嵌入）为 x，触发器文本序列为 τ（长度 K），带触发器的输入表示为 x_trig = x ⊕ τ（append）。
- 训练目标为同时满足：在干净输入上生成正常任务计划 y；在带触发器输入上生成攻击者希望的目标输出 y_t。目标函数（论文给出概念形式）：

  min_{ \hat{W} } E_{x~X} [ L(F(x), y) ]  +  E_{x_trig~X_trig} [ L(F(x_trig), y_t) ]

  其中第一项保证干净输入的正常行为，第二项使触发器激活时输出恶意规划 y_t。

---

## 四、多触发器后门优化（MBO）：核心方法
作者将方法分为两大阶段：1）学习触发器分布（参数化分布）；2）从分布中采样若干触发器并利用它们进行软提示微调注入后门。

### Step 1 — 参数化触发器分布优化（Parametric Trigger Distribution Optimization）
- 目的：学习一个在词表上的分布 π，使得从该分布采样得到的触发词能高效激活后门目标。
- 设计：
  - 触发器长度固定为 K（论文实验中 K=2）。
  - 为每个触发器位置 k 建立一个分类分布 π_{k} over vocabulary（词表大小为 V）。
  - 由于类别分布不可微，采用 Gumbel-Softmax（ST-Gumbel）近似来实现可微分地学习离散 token 分布：
    \tilde{\pi}_{k,i} = softmax((log π_{k,i} + g_i)/T), g_i ~ Gumbel(0,1)
  - 在训练中，随机采样/discretize（straight-through）得到具体 token（触发词），并把这些触发词附加到训练样本，优化目标为让带触发器的输入生成攻击目标 y_t（参见论文中 Eqn.（触发器目标））。

- 结果：得到最优参数化分布 π*，从中可采样多个高效触发器。

图示（触发器学习/方法流程）：
![MBO 方法示意图](https://github.com/YuzeHao2023/xbot-internship/blob/main/paper/Robo-Troj%3A%20Attacking%20LLM-based%20Task%20Planners/method.pdf)

### Step 2 — 多触发器后门插入（Multi-Trigger Backdoor Insertion）
- 从学习得到的 π* 中采样 p 个具体触发器（每个长度 K），构成触发器集 T̄。
- 将这些采样到的触发器以中毒样本的形式加入训练集（即带触发器的输入 x_trig^s = x ⊕ τ^{(i)*}），混合干净样本一起对软提示参数 \hat{W} 进行微调，使得：
  - 干净输入仍然产生正常计划（维持 CDA / 任务成功率）
  - 含任一采样触发器的输入都触发恶意目标计划（高 ASR）
- 训练目标为（论文 Eqn.）：
  min_{\hat{W}} E_{x~X}[L(F(x), y)] + E_{x_trig^s ~ X_trig^s}[ L(F(x_trig^s), y_t) ]
- 仅更新软提示参数（SPT），不改变主模型权重，从而更易于部署（小参数量）且更具隐蔽性。

算法伪代码（论文 Algorithm 1）：
- Step1：初始化 π, \hat{W}，用 Gumbel-Softmax 采样并优化触发器分布与软提示（联合训练/或交替训练）
- Step2：从 π* 采样 p 个触发器，构建中毒数据，混合干净数据训练软提示 \hat{W}，导出最终软提示并部署

---

## 五、部署与攻击触发（Deployment）
- 部署时，受害机器人使用服务器上的冻结 LLM 与训练好的软提示进行推理。平常输入（无触发词）下行为正常，触发器出现时生成恶意动作序列。
- 攻击演示：作者在模拟环境（VirtualHome）与真实机器人平台上展示了触发器被加入任务描述后，规划输出中包含“危险动作”步骤（并可在真实硬件上执行）。

真实机器人演示图（论文给出截图序列）：
![真实机器人演示](https://github.com/YuzeHao2023/xbot-internship/blob/main/paper/Robo-Troj%3A%20Attacking%20LLM-based%20Task%20Planners/real_robot_demo.pdf)

---

## 六、方法的关键优点与设计考虑
- 多触发器（multi-trigger）比单触发器更灵活、更难检测：可以针对不同应用场景使用不同触发词。
- 通过学习触发器分布（参数化）并用 Gumbel-Softmax 优化，使触发器自动化、数据驱动，而非手工挑词。
- 使用 SPT（只微调软提示）使攻击更轻量、易于部署（参数少、易分发），同时保留主模型原能力，提升隐蔽性（高 CDA）。
- 训练目标明确权衡：一方面保持对干净输入的性能，另一方面确保触发器时的攻击成功率（ASR）。

---

## 七、局限与注意点（方法相关）
- 触发器长度、采样数量 p、软提示长度、Gumbel 温度等超参数对攻击效果与隐蔽性影响较大，需要在具体任务上调优。
- 实验侧重家庭任务规划数据集（VirtualHome），跨领域泛化能力有限（作者也在讨论局限）。
- 防御角度：由于只改动软提示而非模型权重，基于权重检测的防御（如剪枝、权重修复）可能失效，需针对 SPT 后门设计新的检测/净化方法。

---
